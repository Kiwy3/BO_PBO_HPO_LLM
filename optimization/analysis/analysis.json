{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 17, "grad_batches": 8, "lora_alpha": 40, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": true, "eval_limit": 10, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.2614035087719298}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 17, "grad_batches": 8, "lora_alpha": 24, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": true, "eval_limit": 10, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.25087719298245614}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 17, "grad_batches": 8, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": true, "eval_limit": 10, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.24385964912280703}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 17, "grad_batches": 8, "lora_alpha": 24, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": true, "eval_limit": 10, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.25087719298245614}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 17, "grad_batches": 8, "lora_alpha": 40, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": true, "eval_limit": 10, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.24385964912280703}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 17, "grad_batches": 8, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": true, "eval_limit": 10, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.24385964912280703}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 8, "lora_alpha": 40, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": true, "eval_limit": 10, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.24385964912280703}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 27, "grad_batches": 8, "lora_alpha": 40, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": true, "eval_limit": 10, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.24736842105263157}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 8, "lora_alpha": 40, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": true, "eval_limit": 10, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.24385964912280703}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 17, "grad_batches": 8, "lora_alpha": 40, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": true, "eval_limit": 10, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.24385964912280703}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 27, "grad_batches": 8, "lora_alpha": 40, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": true, "eval_limit": 10, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.24736842105263157}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 17, "grad_batches": 2, "lora_alpha": 40, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": true, "eval_limit": 10, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.24385964912280703}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 17, "grad_batches": 13, "lora_alpha": 40, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": true, "eval_limit": 10, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.24385964912280703}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 17, "grad_batches": 8, "lora_alpha": 34, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": true, "eval_limit": 10, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.24385964912280703}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 17, "grad_batches": 8, "lora_alpha": 45, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": true, "eval_limit": 10, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.24385964912280703}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 17, "grad_batches": 2, "lora_alpha": 40, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": true, "eval_limit": 10, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.24385964912280703}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 17, "grad_batches": 8, "lora_alpha": 40, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": true, "eval_limit": 10, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.24385964912280703}}
