{
    "hyperparameters": { 
        "learning_rate" : {"min" : -10,"max" : -1,"type" : "exp"},
        "lora_rank" : {"min" : 2,"max" : 32,"type" : "int"},
        "grad_batches" : {"min" : 1,"max" : 16,"type" : "int"},
        "lora_alpha" : {"min" : 16,"max" : 64,"type" : "int"},
        "lora_dropout" : {"min" : 0,"max" : 0.5,"type" : "float"},
        "weight_decay" : {"min" : 0,"max" : 0.5,"type" : "float"} 
        },
    "models":{
        "TinyLlama/TinyLlama-1.1B-Chat-v1.0":"tiny-llama-1.1b",
        "meta-llama/Meta-Llama-3.1-8B":"Llama-3.1-8B",
        "meta-llama/Llama-3.2-3B":"Llama-3.2-3B"

    },
    "experiment": {
        "model_id": "meta-llama/Llama-3.2-3B",
        "model_name": "Llama-3.2-3B",
        "nb_device": 4,
        "epochs": 1,
        "device": "cuda",
        "fast_run": false,
        "eval_limit":0,
        "strategy": "ddp_spawn",
        "algorithm": "iterations",
        "calls": 100,
        "tasks": ["hellaswag"],
        "dataset" : "yahma/alpaca-cleaned",
        "experiment_path": "optimization/experiments/exp04_FT",
        "eval_path": "optimization/experiments/exp04_FT/evaluate",
        "historic_file": "optimization/experiments/exp04_FT/export.json",
        "lora_path" : "checkpoints/lora" 
    }

}
