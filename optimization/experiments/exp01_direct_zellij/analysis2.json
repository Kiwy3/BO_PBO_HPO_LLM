{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 17, "grad_batches": 8, "lora_alpha": 40, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.2531578947368421}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 17, "grad_batches": 8, "lora_alpha": 24, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.23842105263157895}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 17, "grad_batches": 8, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.2531578947368421}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 17, "grad_batches": 8, "lora_alpha": 24, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.23842105263157895}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 17, "grad_batches": 8, "lora_alpha": 40, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.2531578947368421}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 17, "grad_batches": 8, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.2531578947368421}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 8, "lora_alpha": 40, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.2531578947368421}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 27, "grad_batches": 8, "lora_alpha": 40, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.25666666666666665}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 8, "lora_alpha": 40, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.2531578947368421}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 17, "grad_batches": 8, "lora_alpha": 40, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.2531578947368421}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 27, "grad_batches": 8, "lora_alpha": 40, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.25666666666666665}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 8, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26719298245614037}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 27, "grad_batches": 8, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26596491228070174}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 8, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26719298245614037}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 17, "grad_batches": 8, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.2531578947368421}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 27, "grad_batches": 8, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26596491228070174}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 2, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26719298245614037}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 13, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26719298245614037}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 8, "lora_alpha": 50, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.25333333333333335}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 8, "lora_alpha": 61, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26789473684210524}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 8, "lora_alpha": 50, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.25333333333333335}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 8, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26719298245614037}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 8, "lora_alpha": 61, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26789473684210524}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 2, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26719298245614037}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 8, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26719298245614037}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 13, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26719298245614037}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 8, "lora_alpha": 24, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.2531578947368421}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 27, "grad_batches": 8, "lora_alpha": 24, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.24140350877192981}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 8, "lora_alpha": 24, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.2531578947368421}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 17, "grad_batches": 8, "lora_alpha": 24, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.23842105263157895}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 27, "grad_batches": 8, "lora_alpha": 24, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.24140350877192981}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 3, "grad_batches": 2, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.2519298245614035}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 10, "grad_batches": 2, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.2531578947368421}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 3, "grad_batches": 2, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.2519298245614035}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 2, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26719298245614037}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 10, "grad_batches": 2, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.2531578947368421}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 2, "lora_alpha": 61, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26789473684210524}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 13, "lora_alpha": 61, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26789473684210524}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 2, "lora_alpha": 61, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26789473684210524}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 8, "lora_alpha": 61, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26789473684210524}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 13, "lora_alpha": 61, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26789473684210524}}
{"hyperparameters": {"learning_rate": 0.00020346836901064417, "lora_rank": 7, "grad_batches": 2, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26719298245614037}}
{"hyperparameters": {"learning_rate": 0.0820849986238988, "lora_rank": 7, "grad_batches": 2, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26719298245614037}}
{"hyperparameters": {"learning_rate": 0.00020346836901064417, "lora_rank": 7, "grad_batches": 2, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26719298245614037}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 2, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26719298245614037}}
{"hyperparameters": {"learning_rate": 0.0820849986238988, "lora_rank": 7, "grad_batches": 2, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26719298245614037}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 3, "grad_batches": 8, "lora_alpha": 61, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.25333333333333335}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 10, "grad_batches": 8, "lora_alpha": 61, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.2531578947368421}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 3, "grad_batches": 8, "lora_alpha": 61, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.25333333333333335}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 8, "lora_alpha": 61, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26789473684210524}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 10, "grad_batches": 8, "lora_alpha": 61, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.2531578947368421}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 3, "grad_batches": 13, "lora_alpha": 61, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.25333333333333335}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 10, "grad_batches": 13, "lora_alpha": 61, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.2531578947368421}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 3, "grad_batches": 13, "lora_alpha": 61, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.25333333333333335}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 13, "lora_alpha": 61, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26789473684210524}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 10, "grad_batches": 13, "lora_alpha": 61, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.2531578947368421}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 3, "grad_batches": 2, "lora_alpha": 61, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.25333333333333335}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 10, "grad_batches": 2, "lora_alpha": 61, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.2531578947368421}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 3, "grad_batches": 2, "lora_alpha": 61, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.25333333333333335}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 7, "grad_batches": 2, "lora_alpha": 61, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26789473684210524}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 10, "grad_batches": 2, "lora_alpha": 61, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.2531578947368421}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 27, "grad_batches": 2, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26596491228070174}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 27, "grad_batches": 13, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26596491228070174}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 27, "grad_batches": 8, "lora_alpha": 50, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.2694736842105263}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 27, "grad_batches": 8, "lora_alpha": 61, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.2487719298245614}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 27, "grad_batches": 8, "lora_alpha": 50, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.2694736842105263}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 27, "grad_batches": 8, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26596491228070174}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 27, "grad_batches": 8, "lora_alpha": 61, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.2487719298245614}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 27, "grad_batches": 2, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26596491228070174}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 27, "grad_batches": 8, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26596491228070174}}
{"hyperparameters": {"learning_rate": 0.004086771438464067, "lora_rank": 27, "grad_batches": 13, "lora_alpha": 56, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26596491228070174}}
{"hyperparameters": {"learning_rate": 0.00020346836901064417, "lora_rank": 7, "grad_batches": 8, "lora_alpha": 61, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}, "results": {"mmlu": 0.26789473684210524}}
{"hyperparameters": {"learning_rate": 0.0820849986238988, "lora_rank": 7, "grad_batches": 8, "lora_alpha": 61, "lora_dropout": 0.25, "weight_decay": 0.25}, "experiment": {"model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "nb_device": 2, "epochs": 1, "device": "cuda", "fast_run": false, "eval_limit": 100, "calls": 100, "model_name": "tiny-llama-1.1b"}}
