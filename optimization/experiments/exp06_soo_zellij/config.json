{
    "hyperparameters": { 
        "learning_rate" : {"min" : -20,"max" : 0,"type" : "exp"},
        "lora_rank" : {"min" : 2,"max" : 64,"type" : "int"},
        "grad_batches" : {"min" : 1,"max" : 16,"type" : "int"},
        "lora_alpha" : {"min" : 1,"max" : 64,"type" : "float"},
        "lora_dropout" : {"min" : 0,"max" : 0.5,"type" : "float"},
        "weight_decay" : {"min" : 0,"max" : 1,"type" : "float"} 
        },
    "models":{
        "TinyLlama/TinyLlama-1.1B-Chat-v1.0":"tiny-llama-1.1b",
        "meta-llama/Meta-Llama-3.1-8B":"Llama-3.1-8B",
        "meta-llama/Llama-3.2-3B":"Llama-3.2-3B"

    },
    "experiment": {
        "model_id": "meta-llama/Llama-3.2-3B",
        "nb_device": 4,
        "epochs": 1,
        "device": "cuda",
        "fast_run": false,
        "eval_limit":0,
        "strategy": "ddp_spawn",
        "calls": 100,
        "tasks": ["mmlu"],
        "dataset" : "yahma/alpaca-cleaned",
        "experiment_path": "optimization/experiments/exp06_soo_zellij",
        "eval_path": "optimization/experiments/exp06_soo_zellij/evaluate",
        "historic_file": "optimization/experiments/exp06_soo_zellij/export.json",
        "lora_path" : "checkpoints/lora" 
    }
}
